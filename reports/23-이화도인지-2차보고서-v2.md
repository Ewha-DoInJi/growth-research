# Team-Info
| (1) 과제명 | Quantization based on Layer-wise Activation Analysis in Stable Diffusion Models
|:---  |---  |
| (2) 팀 번호 / 팀 이름 | 23-이화도인지 |
| (3) 팀 구성원 | 김도은 (2076035): 리더, *연구 및 논문 작성* <br> 박인애 (2171088): 팀원, *연구 및 논문 작성* <br> 변지은 (2076193) : 팀원, *연구 및 논문 작성*			 |
| (4) 팀 지도교수 | 심재형 교수님 |
| (5) 팀 멘토 | 박준석 멘토님 / 삼성전자 / 수석 연구원 |
| (6) 과제 분류 | 연구 과제 |
| (6) 과제 키워드 | Diffusion model, Quantization, XAI  |
| (7) 과제 내용 요약 | 본 연구는 diffusion model이 timestep마다 서로 다른 layer를 활성화된다는 가설을 기반으로 진행되었습니다. MNIST 데이터셋을 사용하여 기본 diffusion 모델인 DDPM을 1,000 timestep 동안 학습시키고, 각 timestep에서 활성화된 layer를 분석하기 위해 Fisher information 값을 추출하여 활성화 양상을 평가합니다.<br><br>연구의 주요 목표는 diffusion model의 높은 inference time 문제를 개선하는 것입니다. 이를 위해 U-Net 구조에 quantization을 도입하여 연산 속도를 단축하고자 하며, Fisher information 분석 결과를 바탕으로 timestep별 활성화된 layer에 집중적으로 양자화를 적용하는 방식으로 문제를 해결할 계획입니다. 먼저 U-net을 6개 layer로 구분한 후에 각 파트별로 fisher information value 로 timestep마다 활성화되는 layer 및 layer 내의 weight 를 파악합니다. 이와 함께, 정량적 비교를 위해 log scale과 minmax 정규화 기법을 활용하여 정확한 분석을 수행했습니다. 연구 결과, 각 layer의 앞부분 weight가 전반적으로 큰 기여도를 가지며, timestep에 따른 weight 분포의 변화를 살펴본 결과, 초기 timestep에서는 layer의 앞부분 weight 기여도가 높고, 후반 timestep에서는 layer의 뒷부분 weight 기여도가 높아지는 경향을 발견하였습니다. 기존의 전체 layer 동일하게 quantization을 적용하는 방식이 아닌 layer 별로 맞춤형 quantization을 적용함으로써 모델의 속도를 높이되 quantization으로 인한 손실을 최소화하는 성과를 목표로 합니다.<br><br>Quantization 과정에서는 float형 비트 수 감소와 int형 변환을 통해 연산 효율성을 개선합니다. 본 연구는 diffusion model에 XAI 기법을 적용한 새로운 분석 방법을 제시하며, 제안된 quantization 기법이 inference 속도 향상에 기여할 것으로 기대됩니다.|
| (8) 주요 Link | 과제 GIT Address: https://github.com/Ewha-DoInJi/growth-research <br> 과제 보고서: https://github.com/Ewha-DoInJi/growth-research/blob/main/reports/23-이화도인지-1차보고서-v1.md|
| (10) 기타 |  |
<br>

# Project-Summary
| 항목 | 내용 |
|:---  |---  |
| (1) 문제 정의 | Stable Diffusion을 비롯한 현대 이미지 생성형 AI의 주류인 Diffusion 모델은 time step마다 노이즈를 제거하는 과정을 통해 이미지를 생성하는 방식을 채택하고 있습니다. 이 방법은 이미지의 다양성을 높이는 장점이 있지만, 동시에 전체 생성 시간이 길어지는 문제를 안고 있습니다. 이러한 문제는 실시간 응용이나 대규모 이미지 생성 작업에서 주요한 제약 요인으로 작용하며, 성능을 개선하려는 노력이 지속되고 있습니다. Diffusion 모델은 AI 연구자 및 개발자들에게 큰 수혜를 제공할 수 있는 잠재적 기술로 평가받고 있습니다. <br> Diffusion model의 추론 속도를 실제로 확인해보고자, NeurIPs 2023에 게재된 LDM-4를 기반으로 한 PTQD: Accurate Post-Training Quantization for Diffusion Models (NeurIPS 2023) 논문을 바탕으로 코드 구현 및 실험해본 결과, 양자화를 적용하지 않은 diffusion model에서 timestep 250에서는 13초, timestep 에서는 8초의 시간이 소요됨을 확인하였습니다. <br> ![image1](https://github.com/user-attachments/assets/0395ddaa-8899-4ee0-9cc5-3ffa6c8ba7f5) <br> ![image2](https://github.com/user-attachments/assets/3df41fdb-f172-4974-8712-a87a3fdb9558) <br> 1~2초 내로 이미지 생성을 완료하는 GAN 모델과 비교했을 때, Diffusion model은 추론 속도가 느리다는 단점이 있습니다. 이로 인해 Diffusion 모델의 장점을 살리면서도 GAN과 유사한 추론 속도를 목표로 하는 연구가 활발히 진행되고 있습니다. 대표적인 예로 "Diffusion Models Beat GANs on Image Synthesis" (NeurIPS 2021) 논문에서 Diffusion 모델이 이미지 생성 품질에서 GAN을 능가하지만, 속도는 여전히 개선이 필요한 부분으로 지적되고 있습니다. <br> 이와 같은 배경에서 Diffusion 모델의 속도 문제를 해결하고자 양자화 및 최적화 방법을 적용하는 연구는 매우 중요한 주제로 떠오르고 있습니다.|
| (2) 기존 연구와의 비교 | Diffusion 모델에 Quantization을 적용하여 추론 속도를 향상시키는 기존 연구는 다음과 같습니다. 선행 연구 논문은 지도교수님의 추천과 해외 Top-tier 학회 논문을 기준으로 선정되었습니다. <br><br> 1. [Q-Diffusion: Quantizing Diffusion Models (ICCV 2023)](https://arxiv.org/abs/2302.04304) <br> 2. [PTQD: Accurate Post-Training Quantization for Diffusion Models (NeurIPS 2023)](https://arxiv.org/abs/2305.10657) <br> 3. [Temporal Dynamic Quantization for Diffusion Models (NeurIPS 2023)](https://arxiv.org/abs/2306.02316) <br><br> 위의 세 논문은 모두 재학습이 필요 없는 Post-Training Quantization (PTQ) 방식을 사용하여 Diffusion 모델의 다중 timestep 특성에 최적화된 양자화 기법을 제시하고 있습니다. Quantization 과정에서 발생하는 오류 축적 문제는 노이즈 관리 및 분산 조정 등의 방법을 통해 해결하고 있으며, 모델 성능은 FID(Frechet Inception Distance) 점수와 추론 속도로 평가되었습니다. FID 점수는 낮을수록 우수한 성능을 나타냅니다. <br> 저희 팀이 참고한 PTQD 논문에서는 FID 점수가 0.06 정도로 미세하게 증가하였으나, 비트 연산량을 19.9배 감소시키는 성과를 기록하였습니다. 이를 통해 양자화가 추론 속도 향상에 매우 효과적임을 확인할 수 있었습니다. <br> 앞서 언급한 세 논문 모두 Diffusion 모델에 Quantization을 적용하였으며, timestep에 따라 다른 양자화 전략을 적용함으로써 성능 향상을 달성하였습니다. 저희는 이러한 기존 연구의 timestep별 양자화 방식을 채택하는 동시에 XAI 기법 중 하나인 Fisher Information Matrix 기법을 도입하여 layer별로 차별화된 양자화 기법을 적용하는 방식으로 연구의 차별화를 꾀하고자 합니다. 이를 통해 양자화로 인한 오류를 줄이고, 모델의 성능과 효율성을 극대화하는 것을 목표로 합니다. |
| (3) 제안 내용 | 본 연구의 목표는 Stable Diffusion 모델의 추론 속도를 개선하면서도 이미지 품질 저하를 최소화하는 데 있습니다. 모델의 추론 속도 향상을 위해 흔히 사용되는 방법 중 하나는 양자화입니다. 그러나 양자화는 필연적으로 정보 손실을 초래하여 이미지 품질 저하를 야기할 수 있습니다. 이를 해결하기 위하여, 본 연구는 모델 전체에 일률적으로 양자화를 적용하는 대신, 이미지 품질에 상대적으로 낮은 기여도를 보이는 부분에는 보다 강하게 양자화를 적용하고, 이미지 품질에 높은 기여도를 보이는 부분에는 양자화 강도를 줄이는 접근법을 제안합니다. 이러한 차별화된 양자화 적용 방식을 통해 정보 손실을 최소화함으로써 추론 속도의 향상과 동시에 이미지 품질 저하를 최소화할 수 있을 것으로 기대됩니다. <br><br> 본 연구는 diffusion model이 이미지를 생성하는 과정에서 timestep에 따라 layer별 활성화 정도가 달라질 것이라는 가설을 바탕으로 진행되었습니다. 이를 확인하기 위해 timestep에 따른 layer별 활성화 정도를 layer의 weight 값을 비교 분석하여 평가하였습니다. 분석 방법으로는 weight magnitude map의 시각화와 XAI 기법 중 하나인 Fisher Information matrix를 활용한 시각화를 사용하였습니다. 실험 결과를 기반으로 weight의 threshold 값을 설정하였으며, threshold 값보다 작은 weight 값을 가지는 경우에는 양자화를 적용하고, 큰 값을 가지는 경우에는 양자화를 적용하지 않는 부분 양자화 방식을 제안하였습니다. 이를 통해 연산 속도 향상과 이미지 품질 유지 사이의 최적점을 도출하고자 합니다. |
| (4) 기대효과 및 의의 | 제안된 방법을 통해 다음과 같은 효과를 기대할 수 있습니다: <br> 1. 연산 속도의 향상: 선택적 양자화를 통해 전체적인 이미지 생성 시간을 단축할 수 있습니다. <br> 2. 이미지 품질 유지: 중요 레이어의 정밀도를 보존함으로써 기존 양자화 방식 대비 우수한 이미지 품질을 유지할 수 있습니다. |
| (5) 주요 기능 리스트 | Simple Diffusion model, Weight magnitude map, Fisher Information Matrix, Quantization |

<br>
 
# Project-Design & Implementation
| 항목 | 내용 |
|:---  |---  |
| (1) 요구사항 정의 | <img width="1016" alt="Screenshot 2024-11-07 at 11 56 17 PM" src="https://github.com/user-attachments/assets/fe12c62b-f421-4350-aac4-599b4c458eb7"> |
| (2) 전체 시스템 구성 | ![Model-architecture](https://github.com/user-attachments/assets/87f4187f-788a-4b15-b396-6b473e3e0e45) <br> - 첫 번째 단계(파란색): Diffusion model 의 layer information 분석 단계 <br> - 두 번째 단계(핑크색): 양자화 적용 단계 <br><br> 본 연구는 간단한 DDPM 모델을 기반으로 구축된 U-Net을 MNIST 데이터셋에서 1000개의 timestep으로 학습시킨 후, 이를 대상으로 양자화 및 레이어 기여도 분석을 수행합니다. 연구는 크게 두 가지 주요 단계로 이루어집니다. <br><br> **첫 번째 단계: Diffusion Model의 Layer Information 분석** <br> 이 단계에서는 각 timestep에서 어떤 layer가 활성화되고 중요한 역할을 하는지 분석하기 위해 Fisher Information Value를 계산합니다. Fisher Information Value는 특정 weight가 loss 변화에 얼마나 민감하게 반응하는지를 나타내며, 해당 weight의 기여도를 측정하는 지표로 사용됩니다. 이를 통해 특정 timestep에서 중요한 layer와 weight의 중요도 경향성을 분석할 수 있습니다. 분석 결과를 바탕으로 특정 Fisher Information Value 이하의 값을 갖는 layer의 weight를 양자화하기 위한 Threshold를 설정합니다. 이 Threshold는 다양한 경우의 수로 실험하여 최적의 결과를 낼 수 있는 값을 찾습니다. 이 때, 최적의 결과는 속도 향상도와 이미지 품질을 나타내는 FID (Fréchet Inception Distance) Score의 균형을 고려하여 결정됩니다. <br><br> **두 번째 단계: 양자화 적용** <br> 첫 번째 단계에서 도출된 Threshold 값을 기반으로, 각 timestep별로 Diffusion Model의 레이어에 다르게 양자화를 적용합니다. 각 layer 내의 weight의 Fisher Information Value가 Threshold보다 낮을 경우, 해당 weight를 양자화하고, 그렇지 않은 경우에는 양자화를 진행하지 않습니다. 양자화는 PyTorch 라이브러리의 half() 함수를 사용하여, weight 자료형의 비트를 절반으로 줄이는 방식으로 구현됩니다. 이를 통해 32-bit float 자료형을 16-bit half precision으로 변환하여 메모리 사용량을 줄이고 연산 속도를 높이는 효과를 기대할 수 있습니다. <br><br> 이 과정을 통해 Diffusion Model의 성능과 효율성을 최적화하며, 모델의 정보 손실을 최소화하면서도 연산 자원을 효율적으로 사용할 수 있는 연구를 진행할 수 있습니다. |
| (3) 주요엔진 및 기능 설계 | 해당 연구는 Diffusion 모델 기반 U-Net을 사용하여 MNIST 데이터에서 이미지를 생성하는 과정으로, Fisher Information을 통해 레이어와 가중치의 중요도를 분석하고, 이를 바탕으로 모델의 양자화와 성능 최적화를 수행합니다. 모델은 timestep마다 노이즈를 점진적으로 제거하여 이미지를 생성하며, 메모리 사용량을 줄이고 연산 효율을 높이기 위해 mixed precision(양자화)를 적용합니다. <br><br><br> **1) 사용된 주요 라이브러리** <br><br> - **PyTorch**: 모델 생성, 학습, 최적화, 텐서 연산 등 전체적인 딥러닝 파이프라인 구현합니다. <br> - **torchvision**: MNIST 데이터셋 다운로드 및 전처리에 사용합니다. <br> - **einops**: 텐서 형태 변환을 위해 사용하여 모델의 어텐션 및 U-Net 레이어의 텐서 조작을 간편하는 기능을 합니다. <br> - **tqdm**: 학습 및 이미지 생성 과정에서 진행 상황을 시각적으로 표시합니다. <br>  - **matplotlib**: 모델 학습 결과, FID 점수, Fisher Information 분석을 시각화 합니다. <br><br> **2) 주요 모듈 설명 및 역할** <br><br> ***a) 데이터 준비 및 전처리 모듈*** <br> - **기능**: MNIST 데이터를 로드하고 모델에 입력 가능한 텐서 형태로 변환합니다. <br> - **구현 내용**: `datasets.MNIST`와 `DataLoader`를 사용해 데이터를 배치 단위로 로드하고, `transforms.ToTensor()`로 텐서 변환합니다. 데이터를 효과적으로 모델에 전달하여 학습과 생성에 활용합니다. <br><br> ***b) 모델 구성 모듈 (U-Net 및 레이어 구성)*** <br> - **임베딩 모듈** (`SinusoidalEmbeddings`): 각 timestep에 해당하는 임베딩 벡터를 생성해, Diffusion 모델이 시간 정보를 학습하게 합니다. <br> - **Residual 블록** (`ResBlock`): 잔차 연결을 통해 학습 안정성을 높이고 정보 손실을 방지합니다. <br> - **어텐션 모듈** (`Attention`): 특정 레이어에 다중 헤드 어텐션을 추가해 중요도에 따라 가중치를 조절합니다. <br> - **U-Net 레이어** (`UnetLayer`): 업샘플링과 다운샘플링 수행, 필요 시 어텐션 적용합니다. <br> - **전체 U-Net 모델** (`UNET`): 모든 레이어를 연결하여 인코더-디코더 구조를 통해 노이즈 제거 및 이미지 생성합니다. <br><br> ***c) Diffusion 과정 및 스케줄러*** <br> - **스케줄러** (`DDPM_Scheduler`): timestep마다 노이즈를 추가/제거하기 위한 파라미터 α와 β 값을 설정합니다. <br> - **훈련 함수** (`train`): 각 epoch마다 손실을 계산하고, **EMA(Exponential Moving Average)**로 모델의 학습을 안정화하며 점진적으로 최적화합니다. <br><br> ***d) Fisher Information 계산 및 양자화 모듈*** <br><br> - **Fisher Information 계산**: 각 timestep에서 각 레이어의 가중치 중요도를 평가하여, 양자화 대상과 비대상을 결정합니다. <br> - **가중치 크기 분석**: 각 레이어의 가중치 크기를 분석하여 모델의 기여도를 시각화합니다. <br>  - **양자화 적용 및 혼합 정밀도 모델**: `apply_mixed_precision`: threshold 를 기준으로 특정 레이어의 weight는 `float32`로 유지하고, 나머지 weight 를 `float16`으로 변환해 메모리 사용량 절감 및 연산 최적화합니다. `inference_all_half_quant`: 양자화된 모델로 이미지를 생성하고, α와 β 값도 `float16`으로 변환하여 효율성을 높임니다. <br><br>  ***e) 이미지 생성 및 평가 모듈*** <br><br> - **Inference 및 이미지 생성** (`inference`, `inference_all_half_quant`): 노이즈 텐서에서 점진적으로 노이즈를 제거하여 최종 이미지를 생성합니다. <br>  - **FID 점수 계산** (`compute_fid_score`): 실제 이미지와 생성된 이미지 간의 차이를 측정해 모델 성능 평가합니다. FID 계산을 위하여 timm 라이브러리의 ModelEmaV3 를 사용합니다. <br> - **Fisher Information 시각화**: Fisher Information Value를 timestep별로 시각화하여, 각 레이어와 가중치의 중요도를 확인합니다. <br><br><br> **3. 주요 기능 및 데이터 흐름** <br><br> 1. **훈련 단계**: 데이터 준비 → 모델 학습(`train`) → 노이즈 스케줄러를 통한 점진적 학습 → Fisher Information 계산합니다. <br>  2. **Fisher Information 분석 및 양자화**: 각 timestep의 Fisher Information을 계산하여 특정 Threshold 이하의 가중치만 양자화하여 효율성을 향상시킵니다. <br> 3. **Inference 및 시각화**: 양자화된 모델을 사용해 이미지를 생성하고 FID Score로 성능 평가합니다. <br> |
| (4) 주요 기능의 구현 | 기존에는 Latent Diffusion Model(LDM)과 해당 모델을 제안한 [PTQD 논문](https://github.com/ziplab/PTQD)에서 제공한 코드를 바탕으로 실험을 진행하였으나, 오류가 많고 디버깅이 어렵다는 문제점이 있었습니다. 이에 따라, [다음의 글](https://towardsdatascience.com/diffusion-model-from-scratch-in-pytorch-ddpm-9d9760528946)을 참고하여 비교적 구조가 간단한 Simple Diffusion model을 다시 구현하였으며, 해당 모델로 새로운 실험을 진행하였습니다. 기존에는 Saliency map, Layer-LRP, Grad-CAM 등의 다양한 XAI 기법을 활용하여 activation 값과 weight 값의 변화를 통해 layer 활성화도 분석을 진행하였으나, 모델의 안정성과 일괄 적용의 용이성을 고려하여 학습 과정에서 고정된 파라미터인 weight 값을 분석하는 방법으로 수정하였습니다. 이에 따라, Simple diffusion model이 이미지 생성 시 timestepd에 따른 Layer별 weight 값을 분석하였습니다. 실험은 MNIST 데이터셋을 사용하였으며 timestep 1000, 구간(per) 50으로 설정하여 진행되었습니다. 실험 결과는 다음과 같습니다.  <br><br> **1) Layer별 timestep에 따른 weight값 결과** <br> <img width="600" alt="Fisher Information Map per Layers (w:o log scale)" src="https://github.com/user-attachments/assets/de16eda0-e1fc-40d2-896f-6d0925f718ca"> <br> timestep에 따라 유의미한 weight 값의 변화를 보인 것은 Layer 1과 Layer 2 였습니다. 다만 위의 실험결과는 Layer 1과 2가 압도적으로 큰 weight 값을 가지기 때문에 작은 값을 가지는 Layer 3, 4, 5, 6의 값이 0에 수렴하는 형태로 출력되어, log scale을 적용하여 정규화하였습니다. <br><br> <img width="600" alt="Fisher Information Map per Layers (w: log scale)" src="https://github.com/user-attachments/assets/e05c5093-bbc8-47f5-adcc-22a3898cec2a"> <br> 위는 log scale 정규화를 적용한 결과 이미지입니다. 위의 결과 이미지에서도 Layer 1, 2, 6이 비교적 높은 기여도를 가짐을 알 수 있습니다. 또한, Layer 에 구분 없이 전반적으로 weight값의 변화 양상이 유사함을 확인하였습니다. <br><br> **2-1) timestep에 따른 서브모듈별 weight 값** <br> ![fisher_information_grid](https://github.com/user-attachments/assets/85597e1d-7c23-44b5-8448-012d9a4c57c7) <br><br> x축: timestep / y축: weight 값 <br><br> 모든 레이어에서 초반부에 높은 weight 값을, 후반부에서 낮은 weight 값을 가지는 경향성이 확인되었으며, 1)의 실험결과에서 확인한 바와 같이 Layer 1, 2, 6이 비교적 높은 기여도를 갖는 것을 확인할 수 있습니다. <br><br> **2-2) 서브모듈 별 weight 값** <br> (주의: timestep progression이 1.0인 상태(노란색)가 완전한 노이즈 이미지 상태이고, 0.0인 상태가 완성된 이미지 상태에 해당합니다.) <br> ![fisher_information_value_final_gradation](https://github.com/user-attachments/assets/3b7390ff-7524-48da-9b00-7e0900337120) <br> x축: 특정 Layer(Layer 1, 2, ..., 6)에 속한 서브모듈 종류 / y값: weight 값 <br><br> 2-1)의 실험 결과와 같이, Layer 1, 2, 6에 비하여 Layer 3, 4, 5의 weight 기여도가 매우 낮은 것을 확인할 수 있었습니다. 이는 plot 상으로는 유사해보이지만, y축의 값을 보면 weight 값의 차이를 확인할 수 있습니다. 해당 실험 결과를 통해서는 timestep이 1.0에서 0.0으로 진행되는 과정에서 weight 값이 반전되는 구간이 관찰되었습니다. Layer 1의 경우, 완성 이미지에 가까워지는 time step 일수록(0.0에 가까워질수록) ResBlock2.conv2 모듈과 conv 모듈의 weight 기여도가 커지는 것을 확인할 수 있었습니다. Layer 2의 경우 , 완성 이미지에 가까워지는 time step 일수록(0.0에 가까워질수록) ResBlock2.gnorm1, ResBlock2.gnorm2, ResBlock2.conv1, ResBlock2.conv2 의 weight 기여도가 커지는 것을 확인하였으며, 반면에 conv, attention_layer.proj1, attention_layer.proj2 모듈의 weight 기여도는 낮아지는 것을 확인할 수 있었습니다. Layer 6의 경우에도, 완성 이미지에 가까워지는 time step 일수록(0.0에 가까워질수록) ResBlock2.gnorm1, ResBlock2.gnorm2, ResBlock2.conv1, ResBlock2.conv2 의 weight 기여도가 커지는 것을 확인하였으며, 반면에 conv, attention_layer.proj1, attention_layer.proj2 모듈의 weight 기여도는 낮아지는 것을 확인할 수 있었습니다. <br><br> weight 값이 반전되는 구간의 정확한 timestep 파악을 위하여 timestep legend를 추가하여 시각화한 결과 이미지는 다음과 같습니다. <br> ![fisher_information_value_final_legend](https://github.com/user-attachments/assets/0405c207-9d47-4f4f-976c-0721eaa52751) <br><br> 위의 시각화 결과를 토대로, Layer 1에서는 timestep 200 부근에서, Layer 2에서는 timestep 600 부근에서, Layer 3에서는 timestep 550 부근에서, Layer 6에서는 timestep 500 부근에서 weight 값이 반전된다는 것을 확인할 수 있었습니다. <br><br><br><br>  |
| (5) 기타 | 11월 내에 진행되는 학회 및 공모전을 탐색 중에 있으며, 만약 있다면 참가할 예정입니다. |

<br>
